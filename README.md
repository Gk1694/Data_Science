# Table Of Contents:
1. [Data Science Tool](#0)
2. [Data Science Methodologies](#1)

<details><summary>Click to see the Author Name</summary>
 Hello My Name is Ghanshyam
</details>

# Data_Science tool <a id="0" ></a>
## Module 1 Summary
Congratulations! You have completed this module. At this point in the course, you know:

* The Data Science Task Categories include:
  * Data Management -  storage, management and retrieval of data
  * Data Integration and Transformation - streamline data pipelines and automate data processing tasks
  * Data Visualization - provide graphical representation of data and assist with communicating insights
  * Modelling - enable Building, Deployment, Monitoring and Assessment of Data and Machine Learning models
* Data Science Tasks support the following:
  * Code Asset Management - store & manage code, track changes and allow collaborative development
  * Data Asset Management - organize and manage data, provide access control, and backup assets
  * Development Environments - develop, test and deploy code
  * Execution Environments - provide computational resources and run the code

The data science ecosystem consists of many open source and commercial options, and include both traditional desktop applications and server-based tools, as well as cloud-based services that can be accessed using web-browsers and mobile interfaces.

**Data Management Tools:** include Relational Databases, NoSQL Databases, and Big Data platforms:
* MySQL, and PostgreSQL are examples of Open Source Relational Database Management Systems (RDBMS), and IBM Db2 and SQL Server are examples of commercial RDBMSes and are also available as Cloud services.
* MongoDB and Apache Cassandra are examples of NoSQL databases.
* Apache Hadoop and Apache Spark are used for Big Data analytics. 
* Data Integration and Transformation Tools: include Apache Airflow and Apache Kafka. 

**Data Visualization Tools:**  include commercial offerings  such as Cognos Analytics, Tableau and PowerBI  and can be used for building dynamic and interactive dashboards.  

**Code Asset Management Tools:** Git is an essential code asset management tool. GitHub is a popular web-based platform for storing and managing source code. Its features make it an ideal tool for collaborative software development, including version control, issue tracking, and project management. 

**Development Environments:** Popular development environments for Data Science include Jupyter Notebooks and RStudio. 
* Jupyter Notebooks provides an interactive environment for creating and sharing code, descriptive text, data visualizations, and other computational artifacts in a web-browser based interface.  
* RStudio is an integrated development environment (IDE) designed specifically for working with the R programming language, which is a popular tool for statistical computing and data analysis.
  
***

## Module 2 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* You should select a language to learn depending on your needs, the problems you are trying to solve, and whom you are solving them for.
* The popular languages are Python, R, SQL, Scala, Java, C++, and Julia.
* For data science, you can use Python's scientific computing libraries like Pandas, NumPy, SciPy, and Matplotlib. 
* Python can also be used for Natural Language Processing (NLP) using the Natural Language Toolkit (NLTK). 
* Python is open source, and R is free software. 
* R language’s array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background.
* SQL is different from other software development languages because it is a non-procedural language.
* SQL was designed for managing data in relational databases. 
* If you learn SQL and use it with one database, you can apply your SQL knowledge with many other databases easily.
* Data science tools built with Java include Weka, Java-ML, Apache MLlib, and Deeplearning4.
* For data science, popular program built with Scala is Apache Spark which includes Shark, MLlib, GraphX, and Spark Streaming.
* Programs built for Data Science with JavaScript include TensorFlow.js and R-js.
* One great application of Julia for Data Science is JuliaDB.

***

## Module 3 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* Libraries usually contain built-in modules that provide different functionalities.
* You can use data visualization methods to communicate with others and display meaningful results of an analysis. 
* For machine learning, the Scikit-learn library contains tools for statistical modeling, including regression, classification, clustering, and so on.
* Large-scale production of deep-learning models use TensorFlow, a low-level framework. 
* Apache Spark is a general-purpose cluster-computing framework that allows you to process data using compute clusters.
* An application programming interface (API) allows communication between two pieces of software.
* API is the part of the library you see while the library contains all the components of the program. 
* REST APIs allow you to communicate through the internet and take advantage of resources like storage, data, artificially intelligent algorithms, and much more.
* Open data is fundamental to Data Science.
* Community Data License Agreement makes it easier to share open data.
* The IBM Data Asset eXchange (DAX) site contains high-quality open data sets.
* DAX open data sets include tutorial notebooks that provide basic and advanced walk-throughs for developers.
* DAX notebooks open in Watson Studio.
* Machine learning (ML) uses algorithms – also known as “models” – to identify patterns in the data. 
* Types of ML are Supervised, Unsupervised, and Reinforcement. 
* Supervised learning comprises two types of models, regression and classification.
* Deep learning refers to a general set of models and techniques that loosely emulate the way the human brain solves a wide range of problems.
* The Model Asset eXchange is a free, open-source repository for ready-to-use and customizable deep-learning microservices.
* MAX model-serving microservices are built and distributed on GitHub as open-source Docker images.
* You can use Red Hat OpenShift, a Kubernetes platform, to automate deployment, scaling, and management of microservices.
* Ml-exchange.org has multiple predefined models.

***

## Module 4 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* Jupyter Notebooks are used in Data Science for recording experiments and projects.
* Jupyter Lab is compatible with many files and Data Science languages.
* There are different ways to install and use Jupyter Notebooks.
* How to run, delete, and insert a code cell in Jupyter Notebooks.
* How to run multiple notebooks at the same time.
* How to present a notebook using a combination of Markdown and code cells.
* How to shut down your notebook sessions after you have completed your work on them.
* Jupyter implements a two-process model with a kernel and a client.
* The notebook server is responsible for saving and loading the notebooks.
* The kernel executes the cells of code contained in the Notebook. 
* The Jupyter architecture uses the NB convert tool to convert files to other formats.
* Jupyter implements a two-process model with a kernel and a client.
* The Notebook server is responsible for saving and loading the notebooks.
* The Jupyter architecture uses the NB convert tool to convert files to other formats.
* The Anaconda Navigator GUI can launch multiple applications on a local device.
* Jupyter environments in the Anaconda Navigator include JupyterLab and VS Code.
* You can download Jupyter environments separately from the Anaconda Navigator, but they may not be configured properly.
* The Anaconda Navigator GUI can launch multiple applications.
* Additional open-source Jupyter environments include JupyterLab, JupyterLite, VS Code, and Google Colaboratory. 
* JupyterLite is a browser-based tool.

***

## Module 5 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* The capabilities of R and its uses in Data Science.
* The RStudio interface for running R codes. 
* Popular R packages for Data Science.
* Popular data visualization packages in R.
* Plotting with the inbuilt R plot function.
* Plotting with ggplot.
* Adding titles and changing the axis names using the ggtitle and lab’s function.
* A Distributed Version Control System (DVCS) keeps track of changes to code, regardless of where it is stored. 
* Version control allows multiple users to work on the same codebase or repository, mirroring the codebase on their own computers if needed, while the distributed version control software helps manage synchronization amongst the various codebase mirrors.
* Repositories are storage structures that:
  * Store the code
  * Track issues and changes
  * Enable you to collaborate with others
* Git is one of the most popular distributed version control systems. 
* GitHub, GitLab and Bitbucket are examples of hosted version control systems.
* Branches are used to isolate changes to code. When the changes are complete, they can be merged back into the main branch.
* Repositories can be cloned to make it possible to work locally, then sync changes back to the original.

***

# Data Science Methodologies <a id="1" ></a>

## From Problem to approach Lesson Summary:
 In this lesson, you have learned:
* The need to understand and prioritize the business goal.
* The way stakeholder support influences a project.
* The importance of selecting the right model.
* When to use a predictive, descriptive, or classification model.    

## From Requirements to Collection Lesson Summary
 In this lesson, you have learned:
* The significance of defining the data requirements for your model.
* Why the content, format, and representation of your data matter.   
* The importance of identifying the correct sources of data for your project.
* How to handle unavailable and redundant data.
* To anticipate the needs of future stages in the process.

## From Understanding to Prepration Lesson Summary
In this lesson, you have learned:
* The importance of descriptive statistics.
* How to manage missing, invalid, or misleading data.
* The need to clean data and sometimes transform it.
* The consequences of bad data for the model.
* Data understanding is iterative; you learn more about your data the more you study it. 

## From Modeling to prepration Lesson Summary
In this lesson, you have learned:
* The difference between descriptive and predictive models.
* The role of training sets and test sets.
* The importance of asking if the question has been answered.
* Why diagnostic measures tools are needed.
* The purpose of statistical significance tests.
* That modeling and evaluation are iterative processes.

## From Devlopment to Feedback Lesson Summary
In this lesson, you have learned:    
* The importance of stakeholder input.
* To consider the scale of deployment.
* The importance of incorporating feedback to refine the model.
* The refined model must be redeployed.
* This process should be repeated as often as necessary.

