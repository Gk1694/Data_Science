# Table Of Contents:<a id='top' ></a>
1. [Data Science Tool](#0)
2. [Data Science Methodologies](#1)
3. [Database and SQL for Data Science](#2)
4. [Data Anaylsis using python](#3)
5. [Data Visualization with Python](#4)


<details><summary>Click to see the Author Name</summary>
 Hello My Name is Ghanshyam
</details>

# Data_Science tool <a id="0" ></a>
## Module 1 Summary
Congratulations! You have completed this module. At this point in the course, you know:

* The Data Science Task Categories include:
  * Data Management -  storage, management and retrieval of data
  * Data Integration and Transformation - streamline data pipelines and automate data processing tasks
  * Data Visualization - provide graphical representation of data and assist with communicating insights
  * Modelling - enable Building, Deployment, Monitoring and Assessment of Data and Machine Learning models
* Data Science Tasks support the following:
  * Code Asset Management - store & manage code, track changes and allow collaborative development
  * Data Asset Management - organize and manage data, provide access control, and backup assets
  * Development Environments - develop, test and deploy code
  * Execution Environments - provide computational resources and run the code

The data science ecosystem consists of many open source and commercial options, and include both traditional desktop applications and server-based tools, as well as cloud-based services that can be accessed using web-browsers and mobile interfaces.

**Data Management Tools:** include Relational Databases, NoSQL Databases, and Big Data platforms:
* MySQL, and PostgreSQL are examples of Open Source Relational Database Management Systems (RDBMS), and IBM Db2 and SQL Server are examples of commercial RDBMSes and are also available as Cloud services.
* MongoDB and Apache Cassandra are examples of NoSQL databases.
* Apache Hadoop and Apache Spark are used for Big Data analytics. 
* Data Integration and Transformation Tools: include Apache Airflow and Apache Kafka. 

**Data Visualization Tools:**  include commercial offerings  such as Cognos Analytics, Tableau and PowerBI  and can be used for building dynamic and interactive dashboards.  

**Code Asset Management Tools:** Git is an essential code asset management tool. GitHub is a popular web-based platform for storing and managing source code. Its features make it an ideal tool for collaborative software development, including version control, issue tracking, and project management. 

**Development Environments:** Popular development environments for Data Science include Jupyter Notebooks and RStudio. 
* Jupyter Notebooks provides an interactive environment for creating and sharing code, descriptive text, data visualizations, and other computational artifacts in a web-browser based interface.  
* RStudio is an integrated development environment (IDE) designed specifically for working with the R programming language, which is a popular tool for statistical computing and data analysis.
  
***

## Module 2 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* You should select a language to learn depending on your needs, the problems you are trying to solve, and whom you are solving them for.
* The popular languages are Python, R, SQL, Scala, Java, C++, and Julia.
* For data science, you can use Python's scientific computing libraries like Pandas, NumPy, SciPy, and Matplotlib. 
* Python can also be used for Natural Language Processing (NLP) using the Natural Language Toolkit (NLTK). 
* Python is open source, and R is free software. 
* R language’s array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background.
* SQL is different from other software development languages because it is a non-procedural language.
* SQL was designed for managing data in relational databases. 
* If you learn SQL and use it with one database, you can apply your SQL knowledge with many other databases easily.
* Data science tools built with Java include Weka, Java-ML, Apache MLlib, and Deeplearning4.
* For data science, popular program built with Scala is Apache Spark which includes Shark, MLlib, GraphX, and Spark Streaming.
* Programs built for Data Science with JavaScript include TensorFlow.js and R-js.
* One great application of Julia for Data Science is JuliaDB.

***

## Module 3 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* Libraries usually contain built-in modules that provide different functionalities.
* You can use data visualization methods to communicate with others and display meaningful results of an analysis. 
* For machine learning, the Scikit-learn library contains tools for statistical modeling, including regression, classification, clustering, and so on.
* Large-scale production of deep-learning models use TensorFlow, a low-level framework. 
* Apache Spark is a general-purpose cluster-computing framework that allows you to process data using compute clusters.
* An application programming interface (API) allows communication between two pieces of software.
* API is the part of the library you see while the library contains all the components of the program. 
* REST APIs allow you to communicate through the internet and take advantage of resources like storage, data, artificially intelligent algorithms, and much more.
* Open data is fundamental to Data Science.
* Community Data License Agreement makes it easier to share open data.
* The IBM Data Asset eXchange (DAX) site contains high-quality open data sets.
* DAX open data sets include tutorial notebooks that provide basic and advanced walk-throughs for developers.
* DAX notebooks open in Watson Studio.
* Machine learning (ML) uses algorithms – also known as “models” – to identify patterns in the data. 
* Types of ML are Supervised, Unsupervised, and Reinforcement. 
* Supervised learning comprises two types of models, regression and classification.
* Deep learning refers to a general set of models and techniques that loosely emulate the way the human brain solves a wide range of problems.
* The Model Asset eXchange is a free, open-source repository for ready-to-use and customizable deep-learning microservices.
* MAX model-serving microservices are built and distributed on GitHub as open-source Docker images.
* You can use Red Hat OpenShift, a Kubernetes platform, to automate deployment, scaling, and management of microservices.
* Ml-exchange.org has multiple predefined models.

***

## Module 4 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* Jupyter Notebooks are used in Data Science for recording experiments and projects.
* Jupyter Lab is compatible with many files and Data Science languages.
* There are different ways to install and use Jupyter Notebooks.
* How to run, delete, and insert a code cell in Jupyter Notebooks.
* How to run multiple notebooks at the same time.
* How to present a notebook using a combination of Markdown and code cells.
* How to shut down your notebook sessions after you have completed your work on them.
* Jupyter implements a two-process model with a kernel and a client.
* The notebook server is responsible for saving and loading the notebooks.
* The kernel executes the cells of code contained in the Notebook. 
* The Jupyter architecture uses the NB convert tool to convert files to other formats.
* Jupyter implements a two-process model with a kernel and a client.
* The Notebook server is responsible for saving and loading the notebooks.
* The Jupyter architecture uses the NB convert tool to convert files to other formats.
* The Anaconda Navigator GUI can launch multiple applications on a local device.
* Jupyter environments in the Anaconda Navigator include JupyterLab and VS Code.
* You can download Jupyter environments separately from the Anaconda Navigator, but they may not be configured properly.
* The Anaconda Navigator GUI can launch multiple applications.
* Additional open-source Jupyter environments include JupyterLab, JupyterLite, VS Code, and Google Colaboratory. 
* JupyterLite is a browser-based tool.

***

## Module 5 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* The capabilities of R and its uses in Data Science.
* The RStudio interface for running R codes. 
* Popular R packages for Data Science.
* Popular data visualization packages in R.
* Plotting with the inbuilt R plot function.
* Plotting with ggplot.
* Adding titles and changing the axis names using the ggtitle and lab’s function.
* A Distributed Version Control System (DVCS) keeps track of changes to code, regardless of where it is stored. 
* Version control allows multiple users to work on the same codebase or repository, mirroring the codebase on their own computers if needed, while the distributed version control software helps manage synchronization amongst the various codebase mirrors.
* Repositories are storage structures that:
  * Store the code
  * Track issues and changes
  * Enable you to collaborate with others
* Git is one of the most popular distributed version control systems. 
* GitHub, GitLab and Bitbucket are examples of hosted version control systems.
* Branches are used to isolate changes to code. When the changes are complete, they can be merged back into the main branch.
* Repositories can be cloned to make it possible to work locally, then sync changes back to the original.

***

#### [scroll to up](#top)

# Data Science Methodologies <a id="1" ></a>

## From Problem to approach Lesson Summary:
 In this lesson, you have learned:
* The need to understand and prioritize the business goal.
* The way stakeholder support influences a project.
* The importance of selecting the right model.
* When to use a predictive, descriptive, or classification model.    

## From Requirements to Collection Lesson Summary
 In this lesson, you have learned:
* The significance of defining the data requirements for your model.
* Why the content, format, and representation of your data matter.   
* The importance of identifying the correct sources of data for your project.
* How to handle unavailable and redundant data.
* To anticipate the needs of future stages in the process.

## From Understanding to Prepration Lesson Summary
In this lesson, you have learned:
* The importance of descriptive statistics.
* How to manage missing, invalid, or misleading data.
* The need to clean data and sometimes transform it.
* The consequences of bad data for the model.
* Data understanding is iterative; you learn more about your data the more you study it. 

## From Modeling to prepration Lesson Summary
In this lesson, you have learned:
* The difference between descriptive and predictive models.
* The role of training sets and test sets.
* The importance of asking if the question has been answered.
* Why diagnostic measures tools are needed.
* The purpose of statistical significance tests.
* That modeling and evaluation are iterative processes.

## From Devlopment to Feedback Lesson Summary
In this lesson, you have learned:    
* The importance of stakeholder input.
* To consider the scale of deployment.
* The importance of incorporating feedback to refine the model.
* The refined model must be redeployed.
* This process should be repeated as often as necessary.

***

#### [scroll to up](#top)

# Database and SQL for Data Science <a id="2" ></a>

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know: 
* You can use Data Manipulation Language (DML) statements to read and modify data. 
* The search condition of the WHERE clause uses a predicate to refine the search. 
* COUNT, DISTINCT, and LIMIT are expressions that are used with SELECT statements. 
* INSERT, UPDATE, and DELETE are DML statements for populating and changing tables.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know: 
* A database is a repository of data that provides functionality for adding, modifying, and querying the data. 
* SQL is a language used to query or retrieve data from a relational database. 
* The Relational Model is the most used data model for databases because it allows for data independence. 
* The primary key of a relational table uniquely identifies each tuple or row, preventing duplication of data and providing a way of 
  defining relationships between tables. 
* SQL statements fall into two different categories: Data Definition Language (DDL) statements and Data Manipulation Language (DML) 
  statements.
  
## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* You can use the WHERE clause to refine your query results.
* You can use the wildcard character (%) as a substitute for unknown characters in a pattern.
* You can use BETWEEN ... AND ... to specify a range of numbers.
* You can sort query results into ascending or descending order, using the ORDER BY clause to specify the column to sort on.
* You can group query results by using the GROUP BY clause.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* Most databases come with built-in functions that you can use in SQL statements to perform operations on data within the database itself.
* When you work with large datasets, you may save time by using built-in functions rather than first retrieving the data into your 
  application and then executing functions on the retrieved data.
* You can use sub-queries to form more powerful queries than otherwise.
* You can use a sub-select expression to evaluate some built-in aggregate functions like the average function. 
* Derived tables or table expressions are sub-queries where the outer query uses the results of the sub-query as a data source.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* You can access a database from a language like Python by using the appropriate API. Examples include ibm_db API for IBM DB2, psycopg2 
  for ProstgreSQL, and dblib API for SQL Server.
* DB-API is Python's standard API for accessing relational databases. It allows you to write a single program that works with multiple 
  kinds of relational databases instead of writing a separate program for each one.
* The DB_API  connect constructor creates a connection to the database and returns a Connection Object, which is then used by the various 
  connection methods.
* The connection methods are:
  - The cursor() method, which returns a new cursor object using the connection.
  - The commit() method, which is used to commit any pending transaction to the database.
  - The rollback() method, which causes the database to roll-back to the start of any pending transaction.
  - The close() method, which is used to close a database connection. 
* You can use SQL Magic commands to execute queries more easily from Jupyter Notebooks. 
  - Magic commands have the general format %sql select * from tablename.
  - Cell magics start with a double %% (percent) sign and apply to the entire cell.
  - Line magics start with a single % (percent) sign and apply to a particular line in a cell.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* Views are a dynamic mechanism for presenting data from one or more tables.A transaction represents a complete unit of work, which can be 
  one or more SQL statements.
* An ACID transaction is one where all the SQL statements must complete successfully, or none at all.
* A stored procedure is a set of SQL statements that are stored and executed on the database server, allowing you to send one statement as 
  an alternative to sending multiple statements.
* You can write stored procedures in many different languages like SQL PL, PL/SQL, Java, and C.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* A join combines the rows from two or more tables based on a relationship between certain columns in these tables.
* To combine data from three or more different tables, you simply add new joins to the SQL statement. 
* There are two types of table joins: inner join and outer join; and three types of outer joins: left outer join, right outer join, and 
  full outer join. 
* The most common type of join is the inner join, which matches the results from two tables and returns only the rows that match.
* You can use an alias as shorthand for a table or column name.
* You can use a self-join to compare rows within the same table.

    ***
  
#### [scroll to up](#top)

# Data Analysis using python <a id="3"></a>

## Lesson Summary
In this lesson, you have learned how to:
* **Define the Business Problem:** Look at the data and make some high-level decision on what kind of analysis should be done
* **Import and Export Data in Python:** How to import data from multiple data sources using the Pandas library and how to export files 
  into different formats.
* **Analyze Data in Python:** How to do some introductory analysis in Python using functions like dataframe.head() to view the first few 
  lines of the dataset, dataframe.info() to view the column names and data types.

## Lesson Summary
In this lesson, you have learned how to:
* **Identify and Handle Missing Values:** Drop rows with incomplete information and impute missing data using the mean values.
* **Understand Data Formatting:** Wrangle features in a dataset and make them meaningful for data analysis.
* **Apply normalization to a data set:** By understanding the relevance of using feature scaling on your data and how normalization and 
  standardization have varying effects on your data analysis.

## Lesson Summary
In this lesson, you have learned how to:
* **Describe Exploratory Data Analysis:** By summarizing the main characteristics of the data and extracting valuable insights.
* **Compute basic descriptive statistics:** Calculate the mean, median, and mode using python and use it as a basis in understanding the 
  distribution of the data.
* **Create data groups:** How and why you put continuous data in groups and how to visualize them.
* **Define correlation as the linear association between two numerical variables:** Use Pearson correlation as a measure of the 
  correlation between two continuous variables
* **Define the association between two categorical variables:** Understand how to find the association of two variables using the Chi- 
  square test for association and how to interpret them.

## Lesson Summary
In this lesson, you have learned how to:
* **Define the explanatory variable and the response variable:** Define the response variable (y) as the focus of the experiment and the 
  explanatory variable (x) as a variable used to explain the change of the response variable. Understand the differences between Simple 
  Linear Regression because it concerns the study of only one explanatory variable and Multiple Linear Regression because it concerns the 
  study of two or more explanatory variables.
* **Evaluate the model using Visualization:** By visually representing the errors of a variable using scatterplots and interpreting the 
  results of the model.
* **Identify alternative regression approaches:** Use a Polynomial Regression when the Linear regression does not capture the curvilinear 
  relationship between variables and how to pick the optimal order to use in a model.
* **Interpret the R-square and the Mean Square Error:** Interpret R-square (x 100) as the percentage of the variation in the response 
  variable y  that is explained by the variation in explanatory variable(s) x. The Mean Squared Error tells you how close a regression 
  line is to a set of points. It does this by taking the average distances from the actual points to the predicted points and squaring 
  them.

## Lesson Summary
In this lesson, you have learned how to:
* **Identify over-fitting and under-fitting in a predictive model:** Overfitting occurs when a function is too closely fit to the training 
  data points and captures the noise of the data. Underfitting refers to a model that can't model the training data or capture the trend 
  of the data.
* **Apply Ridge Regression to linear regression models:** Ridge regression is a regression that is employed in a Multiple regression model 
  when Multicollinearity occurs.
* **Tune hyper-parameters of an estimator using Grid search:** Grid search is a time-efficient tuning technique that exhaustively computes 
  the optimum values of hyperparameters performed on specific parameter values of estimators.

#### [Scroll To Top](#top)

# Data Visualization with Python <a id="4" ></a>

## Module 1
**Title:** Introduction to Data Visualization Tools

**Description**

Data visualization is a way of presenting complex data in a form that is graphical and easy to understand. When analyzing large volumes of data and making data-driven decisions, data visualization is crucial. In this module, you will learn about data visualization and some key best practices to follow when creating plots and visuals. You will discover the history and the architecture of Matplotlib. Furthermore, you will learn about basic plotting with Matplotlib and explore the dataset on Canadian immigration, which you will use during the course. Lastly, you will analyze data in a data frame and generate line plots using Matplotlib.

**Objectives**

By the end of this week, you will be able to:
* Discuss data visualization and its importance
* Discover the history of Matplotlib and its architecture
* Use Matplotlib to create plots employing Jupyter notebook
* Explore the dataset on immigration to Canada
* Identify the steps to analyze data in Pandas data frame
* Use Matplotlib to create line plots

**Activities**

Lesson 0: Welcome to the Course
* Welcome to the Course
* How to Make the Most of this Course
* Syllabus
Lesson 1: Introduction to Data Visualization
* Overview of Data Visualization
* Types of Plots
* Plot Libraries
* Introduction to Matplotlib
* Basic Plotting with Matplotlib
* Dataset on Immigration to Canada
* Line Plots
* Hands-on Lab: Exploring and Pre-processing a Dataset using Pandas
* Hands-on Lab: Introduction to Matplotlib and Line Plots
* Practice Quiz: Introduction to Data Visualization 
* Module 1 Summary: Introduction to Data Visualization Tools
* Module 1 Cheat Sheet
* Module 1 Graded Quiz: Introduction to Data Visualization Tools

**Summary:** Introduction to Data Visualization Tools

Congratulations! You have completed this module. At this point in the course, you know: 
* Data visualization is the process of presenting data in a visual format, such as charts, graphs, and maps, to help people understand and 
  analyze data easily. 
* Data visualization has diverse use cases, such as in business, science, healthcare, and finance. 
* It is important to follow best practices, such as selecting appropriate visualizations for the data being presented, choosing colors and 
  fonts that are easy to read and interpret, and minimizing clutter.
* There are various types of plots commonly used in data visualization.
* Line plots capture trends and changes over time, allowing us to see patterns and fluctuations.
* Bar plots compare categories or groups, providing a visual comparison of their values.
* Scatter plots explore relationships between variables, helping us identify correlations or trends.
* Box plots display the distribution of data, showcasing the median, quartiles, and outliers.
* Histograms illustrate the distribution of data within specific intervals, allowing us to understand its shape and concentration.
* Matplotlib is a plotting library that offers a wide range of plotting capabilities.
* Pandas is a plotting library that provides Integrated plotting functionalities for data analysis.
* Seaborn is a specialized library for statistical visualizations, offering attractive default aesthetics and color palettes.
* Folium is a Python library that allows you to create interactive and customizable maps.
* Plotly is an interactive and dynamic library for data visualization that supports a wide range of plot types and interactive features.
* PyWaffle enables you to visualize proportional representation using squares or rectangles.
* Matplotlib is one of the most widely used data visualization libraries in Python. 
* Matplotlib was initially developed as an EEG/ECoG visualization tool. 
* Matplotlib’s architecture is composed of three main layers: Backend layer, Artist layer, and the Scripting layer. 
* The anatomy of a plot refers to the different components and elements that make up a visual representation of data.
* Matplotlib is a well-established data visualization library that can be integrated in different environments. 
* Jupyter Notebook is an open-source web application that allows you to create and share documents.
* Matplotlib has a number of different backends available. 
* You can easily include the label and title to your plot with plt.
* In order to start creating different types of plots of the data, you will need to import the data into a Pandas DataFrame.
* A line plot is a plot in the form of a series of data points connected by straight line segments. 
* Line plot is one of the most basic type of chart and is common in many fields. 
* You can generate a line plot by assigning "line" to 'Kind' parameter in the plot() function.



## Module 2
**Title:** Basic and Specialized Visualization Tools

**Description**

Visualization tools play a crucial role in data analysis and communication. These are essential for extracting insights and presenting information in a concise manner to both technical and non-technical audiences. In this module, you will create a diverse range of plots using Matplotlib, the data visualization library. Throughout this module, you will learn about area plots, histograms, bar charts, pie charts, box plots, and scatter plots. You will also explore the process of creating these visualization tools using Matplotlib.

**Objectives**

By the end of this week, you will be able to:
* Explore an area plot with an illustration and create it using Matplotlib
* Define a histogram with an illustration and create it using Matplotlib
* Describe a bar chart with an illustration and create it using Matplotlib
* Discover a pie chart with an illustration and create it using Matplotlib
* Describe a box plot with an illustration and create it using Matplotlib
* Discover a scatter plot with an illustration and create it using Matplotlib

**Activities**

Lesson 1: Basic Visualization Tools
* Area Plots
* Histograms
* Bar Charts
* Hands-on Lab: Area Plots, Histograms, and Bar Charts
* Practice Quiz: Basic Visualization Tools
* Lesson 2: Specialized Visualization Tools
* Pie Charts
* Box Plots
* Scatter Plots
* Hands-on Lab: Pie Charts, Box Plots, Scatter Plots, and Bubble Plots
* Plotting Directly with Matplotlib
* Hands-on Lab: Plotting Directly with Matplotlib
* Practice Quiz: Specialized Visualization Tools
* Module 2 Summary: Basic and Specialized Visualization Tools
* Module 2 Cheat Sheet
* Module 2 Graded Quiz: Basic and Specialized Visualization Tools

**Summary:** Basic and Specialized Visualization Tools

Congratulations! You have completed this module. At this point in the course, you know: 
* A pie chart is a circular statistical graphic, divided into segments, to illustrate numerical proportion.
* The process of creating a pie chart involves importing Matplotlib to represent a large set of data over a period of time.
* A box plot is a way of statistically representing given data distribution through five main dimensions.
* The five main dimensions are minimum, first quartile, median, third quartile, and maximum.
* You can create a box plot using Matplotlib.
* A scatter plot displays values pertaining to typically two variables against each other.
* The process of creating a scatter plot involves importing Matplotlib to visualize a large set of data.
* Matplotlib is a versatile plotting library that offers a flexible interface for creating various types of plots.
* Matplotlib’s Pyplot module offers a convenient way to create and customize plots quickly.
* Data Storytelling is the ‘art of storytelling’ that involves creating a narrative around the data.
* Data visualization is an important aspect of data storytelling and involves creating engaging visuals.

## Module 3
**Title:** Advanced Visualizations and Geospatial Data

**Description**

Advanced visualization tools are sophisticated platforms that provide a wide range of advanced features and capabilities. These tools provide an extensive set of options that help create visually appealing and interactive visualizations. In this module, you will learn about waffle charts and word cloud including their application. You will explore Seaborn, a new visualization library in Python, and learn how to create regression plots using it. In addition, you will learn about folium, a data visualization library that visualizes geospatial data. Furthermore, you will explore the process of creating maps using Folium and superimposing them with markers to make them interesting. Finally, you will learn how to create a Choropleth map using Folium.

**Objectives**

By the end of this week, you will be able to:
* Explore waffle charts and word cloud along with their application
* Describe Seaborn and explore the process of generating attractive regression plots
* Describe Folium and explore the process of creating maps
* Explore the process of superimposing markers on maps using Foilum
* Describe Choropleth maps with the help of an illustration
* Explore the process of creating a Choropleth map using Folium

**Activities**

Lesson 1: Advanced Visualizations and Geospatial Data
* Waffle Charts & Word Cloud
* Seaborn and Regression Plots
* Hands-on Lab: Waffle Charts, Word Clouds, and Regression Plots
* Practice Quiz: Advanced Visualization Tools
* Lesson 2: Visualizing Geospatial Data
* Introduction to Folium
* Maps with Markers
* Choropleth Maps
* Hands-on Lab: Creating Maps and Visualizing Geospatial Data
* Practice Quiz: Visualizing Geospatial Data
* Module 3 Summary: Advanced Visualizations and Geospatial Data
* Module 3 Cheat Sheet
* Module 3 Graded Quiz: Advanced Visualizations and Geospatial Data

**Summary:** Advanced Visualizations and Geospatial Data
Congratulations! You have completed this module. At this point in the course, you know: 
* Folium is a data visualization library in Python that helps people visualize geospatial data. 
* With Folium, you can create maps of different styles, such as street-level maps, stamen maps, and more. 
* A feature of Folium is that you can create different map styles using the tiles parameter.
* With Folium, you can easily add markers on maps.
* The ‘location’ parameter specifies the latitude and longitude coordinates of the center point of the map.
* Markers play a vital role in enhancing interactivity and adding context to maps.
* The folium.Marker() function specifies location parameters.
* The popup parameter provides a label upon being clicked.
* Markers can be created using “feature group.”
* A choropleth map is a thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical variable.
* When creating a choropleth map, Folium requires a GeoJson file that includes geospatial data of the region.
* The Mapbox Bright Tileset displays the name of every country when used on a map.

## Module 4
**Title:** Creating Dashboards with Plotly and Dash

**Description**

Dashboards and interactive data applications are crucial tools for data visualization and analysis because they provide a consolidated view of key data and metrics in a visually appealing and understandable format. In this module, you will explore the benefits of dashboards and identify the different web-based dashboarding tools in Python. You will learn about Plotly and discover how to use Plotly graph objects and Plotly express to create charts. You will gain insight into Dash, an open-source user interface Python library, and its two components. Finally, you will gain a clear understanding of the callback function and determine how to connect core and HTML components using callback.

**Objectives**

By the end of this week, you will be able to:
* Identify different web-based dashboarding tools available in Python
* Explore Plotly and its two sub-modules
* Use Plotly graph objects and Plotly express to create charts
* Discover Dash and its two components
* Describe the callback function
* Determine the process of connecting core and HTML components using callback

**Activities**

Lesson 1: Creating Dashboards with Plotly and Dash
* Dashboarding Overview
* Additional Resources for Dashboards
* Introduction to Plotly
* Additional Resources for Plotly
* Plotly Basics: Scatter, Line, Bar, Bubble, Histogram, Pie, Sunburst
* Practice Quiz: Creating Dashboards with Plotly
* Lesson 2: Working with Dash
* Introduction to Dash
* Overview of Cloud IDE lab environment
* Dash Basics: HTML and Core Components
* Additional Resources for Dash
* Make Dashboards Interactive
* Additional Resources for Interactive Dashboards
* Add Interactivity: User Inputs and Callbacks
* Understanding the Lab Environment
* Flight Delay Time Statistics Dashboard
* Practice Quiz: Working with Dash
* Module 4 Summary: Creating Dashboards with Plotly and Dash
* Module 4 Cheat Sheet
* Module 4 Graded Quiz: Creating Dashboards with Plotly and Dash

**Summary:** Creating Dashboards with Plotly and Dash
Congratulations! You have completed this module. At this point in the course, you know: 
* Dash is an Open-Source User Interface Python library for creating reactive, web-based applications.
* It is easy to build Graphical User Interfaces using Dash as it abstracts all technologies required to make the applications.
* There are two components of Dash: Core and HTML components.
* The dash_core_components describe higher-level interactive components generated with JavaScript, HTML, and CSS through the React.js 
  library.
* The dash_html_components library has a component for every HTML tag.
* A callback function is a python function that is automatically called by Dash whenever an input component's property changes.
* The @app.callback decorator decorates the callback function in order to tell Dash to call it whenever there is a change in the input 
  component value.
* The callback function takes input and output components as parameters and performs operations to return the desired result for the 
  output component.

**Module 5**

**Title:** Final Project and Exam

**Description**

The primary focus of this module is to practice the skills gained earlier in the course and then demonstrate those skills in your final assignment. For the final assignment you will analyze historical automobile sales data covering periods of recession and non-recession. You will bring your analysis to life using visualization techniques and then display the plots and graphs on dashboards. Finally, you will submit your assignment for peer review and you will review an assignment from one of your peers. To wrap up the course you will take a final exam in the form of a timed quiz.

**Objectives**

By the end of this week, you will be able to:
* Practice visualization skills
* Practice creating a dashboard
* Create various visualizations using a number of plot libraries
* Create a dashboard and add interactivity
* Review and grade an assignment submitted by peers

**Activities**

* Lesson 1: Practice Project
* Practice Project Overview
* Practice Assignment: Part 1 -  Analyzing wildfire data in Australia
* Practice Assignment: Part 2 - Creating Dashboards
* Lesson 2: Final Project
* Final Project Overview
* Final Assignment: Part 1 - Create Visualizations using Matplotlib, Seaborn & Folium
* Final Assignment: Part 2 - Create Dashboard with Plotly and Dash
* Final Assignment: Part 3 - Submission and Grading
* Final Exam: Data Visualization with Python - Timed Quiz
* Lesson 3: Course Wrap Up
* Course Summary


#### [scroll to up](#top)
