# Table Of Contents:<a id='top' ></a>
1. [Data Science Tool](#0)
2. [Data Science Methodologies](#1)
3. [Database and SQL for Data Science](#2)
4. [Data Anaylsis using python](#3)


<details><summary>Click to see the Author Name</summary>
 Hello My Name is Ghanshyam
</details>

# Data_Science tool <a id="0" ></a>
## Module 1 Summary
Congratulations! You have completed this module. At this point in the course, you know:

* The Data Science Task Categories include:
  * Data Management -  storage, management and retrieval of data
  * Data Integration and Transformation - streamline data pipelines and automate data processing tasks
  * Data Visualization - provide graphical representation of data and assist with communicating insights
  * Modelling - enable Building, Deployment, Monitoring and Assessment of Data and Machine Learning models
* Data Science Tasks support the following:
  * Code Asset Management - store & manage code, track changes and allow collaborative development
  * Data Asset Management - organize and manage data, provide access control, and backup assets
  * Development Environments - develop, test and deploy code
  * Execution Environments - provide computational resources and run the code

The data science ecosystem consists of many open source and commercial options, and include both traditional desktop applications and server-based tools, as well as cloud-based services that can be accessed using web-browsers and mobile interfaces.

**Data Management Tools:** include Relational Databases, NoSQL Databases, and Big Data platforms:
* MySQL, and PostgreSQL are examples of Open Source Relational Database Management Systems (RDBMS), and IBM Db2 and SQL Server are examples of commercial RDBMSes and are also available as Cloud services.
* MongoDB and Apache Cassandra are examples of NoSQL databases.
* Apache Hadoop and Apache Spark are used for Big Data analytics. 
* Data Integration and Transformation Tools: include Apache Airflow and Apache Kafka. 

**Data Visualization Tools:**  include commercial offerings  such as Cognos Analytics, Tableau and PowerBI  and can be used for building dynamic and interactive dashboards.  

**Code Asset Management Tools:** Git is an essential code asset management tool. GitHub is a popular web-based platform for storing and managing source code. Its features make it an ideal tool for collaborative software development, including version control, issue tracking, and project management. 

**Development Environments:** Popular development environments for Data Science include Jupyter Notebooks and RStudio. 
* Jupyter Notebooks provides an interactive environment for creating and sharing code, descriptive text, data visualizations, and other computational artifacts in a web-browser based interface.  
* RStudio is an integrated development environment (IDE) designed specifically for working with the R programming language, which is a popular tool for statistical computing and data analysis.
  
***

## Module 2 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* You should select a language to learn depending on your needs, the problems you are trying to solve, and whom you are solving them for.
* The popular languages are Python, R, SQL, Scala, Java, C++, and Julia.
* For data science, you can use Python's scientific computing libraries like Pandas, NumPy, SciPy, and Matplotlib. 
* Python can also be used for Natural Language Processing (NLP) using the Natural Language Toolkit (NLTK). 
* Python is open source, and R is free software. 
* R language’s array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background.
* SQL is different from other software development languages because it is a non-procedural language.
* SQL was designed for managing data in relational databases. 
* If you learn SQL and use it with one database, you can apply your SQL knowledge with many other databases easily.
* Data science tools built with Java include Weka, Java-ML, Apache MLlib, and Deeplearning4.
* For data science, popular program built with Scala is Apache Spark which includes Shark, MLlib, GraphX, and Spark Streaming.
* Programs built for Data Science with JavaScript include TensorFlow.js and R-js.
* One great application of Julia for Data Science is JuliaDB.

***

## Module 3 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* Libraries usually contain built-in modules that provide different functionalities.
* You can use data visualization methods to communicate with others and display meaningful results of an analysis. 
* For machine learning, the Scikit-learn library contains tools for statistical modeling, including regression, classification, clustering, and so on.
* Large-scale production of deep-learning models use TensorFlow, a low-level framework. 
* Apache Spark is a general-purpose cluster-computing framework that allows you to process data using compute clusters.
* An application programming interface (API) allows communication between two pieces of software.
* API is the part of the library you see while the library contains all the components of the program. 
* REST APIs allow you to communicate through the internet and take advantage of resources like storage, data, artificially intelligent algorithms, and much more.
* Open data is fundamental to Data Science.
* Community Data License Agreement makes it easier to share open data.
* The IBM Data Asset eXchange (DAX) site contains high-quality open data sets.
* DAX open data sets include tutorial notebooks that provide basic and advanced walk-throughs for developers.
* DAX notebooks open in Watson Studio.
* Machine learning (ML) uses algorithms – also known as “models” – to identify patterns in the data. 
* Types of ML are Supervised, Unsupervised, and Reinforcement. 
* Supervised learning comprises two types of models, regression and classification.
* Deep learning refers to a general set of models and techniques that loosely emulate the way the human brain solves a wide range of problems.
* The Model Asset eXchange is a free, open-source repository for ready-to-use and customizable deep-learning microservices.
* MAX model-serving microservices are built and distributed on GitHub as open-source Docker images.
* You can use Red Hat OpenShift, a Kubernetes platform, to automate deployment, scaling, and management of microservices.
* Ml-exchange.org has multiple predefined models.

***

## Module 4 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* Jupyter Notebooks are used in Data Science for recording experiments and projects.
* Jupyter Lab is compatible with many files and Data Science languages.
* There are different ways to install and use Jupyter Notebooks.
* How to run, delete, and insert a code cell in Jupyter Notebooks.
* How to run multiple notebooks at the same time.
* How to present a notebook using a combination of Markdown and code cells.
* How to shut down your notebook sessions after you have completed your work on them.
* Jupyter implements a two-process model with a kernel and a client.
* The notebook server is responsible for saving and loading the notebooks.
* The kernel executes the cells of code contained in the Notebook. 
* The Jupyter architecture uses the NB convert tool to convert files to other formats.
* Jupyter implements a two-process model with a kernel and a client.
* The Notebook server is responsible for saving and loading the notebooks.
* The Jupyter architecture uses the NB convert tool to convert files to other formats.
* The Anaconda Navigator GUI can launch multiple applications on a local device.
* Jupyter environments in the Anaconda Navigator include JupyterLab and VS Code.
* You can download Jupyter environments separately from the Anaconda Navigator, but they may not be configured properly.
* The Anaconda Navigator GUI can launch multiple applications.
* Additional open-source Jupyter environments include JupyterLab, JupyterLite, VS Code, and Google Colaboratory. 
* JupyterLite is a browser-based tool.

***

## Module 5 Summary
Congratulations! You have completed this module. At this point in the course, you know:
* The capabilities of R and its uses in Data Science.
* The RStudio interface for running R codes. 
* Popular R packages for Data Science.
* Popular data visualization packages in R.
* Plotting with the inbuilt R plot function.
* Plotting with ggplot.
* Adding titles and changing the axis names using the ggtitle and lab’s function.
* A Distributed Version Control System (DVCS) keeps track of changes to code, regardless of where it is stored. 
* Version control allows multiple users to work on the same codebase or repository, mirroring the codebase on their own computers if needed, while the distributed version control software helps manage synchronization amongst the various codebase mirrors.
* Repositories are storage structures that:
  * Store the code
  * Track issues and changes
  * Enable you to collaborate with others
* Git is one of the most popular distributed version control systems. 
* GitHub, GitLab and Bitbucket are examples of hosted version control systems.
* Branches are used to isolate changes to code. When the changes are complete, they can be merged back into the main branch.
* Repositories can be cloned to make it possible to work locally, then sync changes back to the original.

***

# Data Science Methodologies <a id="1" ></a>

## From Problem to approach Lesson Summary:
 In this lesson, you have learned:
* The need to understand and prioritize the business goal.
* The way stakeholder support influences a project.
* The importance of selecting the right model.
* When to use a predictive, descriptive, or classification model.    

## From Requirements to Collection Lesson Summary
 In this lesson, you have learned:
* The significance of defining the data requirements for your model.
* Why the content, format, and representation of your data matter.   
* The importance of identifying the correct sources of data for your project.
* How to handle unavailable and redundant data.
* To anticipate the needs of future stages in the process.

## From Understanding to Prepration Lesson Summary
In this lesson, you have learned:
* The importance of descriptive statistics.
* How to manage missing, invalid, or misleading data.
* The need to clean data and sometimes transform it.
* The consequences of bad data for the model.
* Data understanding is iterative; you learn more about your data the more you study it. 

## From Modeling to prepration Lesson Summary
In this lesson, you have learned:
* The difference between descriptive and predictive models.
* The role of training sets and test sets.
* The importance of asking if the question has been answered.
* Why diagnostic measures tools are needed.
* The purpose of statistical significance tests.
* That modeling and evaluation are iterative processes.

## From Devlopment to Feedback Lesson Summary
In this lesson, you have learned:    
* The importance of stakeholder input.
* To consider the scale of deployment.
* The importance of incorporating feedback to refine the model.
* The refined model must be redeployed.
* This process should be repeated as often as necessary.

***

# Database and SQL for Data Science <a id="2" ></a>

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know: 
* You can use Data Manipulation Language (DML) statements to read and modify data. 
* The search condition of the WHERE clause uses a predicate to refine the search. 
* COUNT, DISTINCT, and LIMIT are expressions that are used with SELECT statements. 
* INSERT, UPDATE, and DELETE are DML statements for populating and changing tables.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know: 
* A database is a repository of data that provides functionality for adding, modifying, and querying the data. 
* SQL is a language used to query or retrieve data from a relational database. 
* The Relational Model is the most used data model for databases because it allows for data independence. 
* The primary key of a relational table uniquely identifies each tuple or row, preventing duplication of data and providing a way of 
  defining relationships between tables. 
* SQL statements fall into two different categories: Data Definition Language (DDL) statements and Data Manipulation Language (DML) 
  statements.
  
## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* You can use the WHERE clause to refine your query results.
* You can use the wildcard character (%) as a substitute for unknown characters in a pattern.
* You can use BETWEEN ... AND ... to specify a range of numbers.
* You can sort query results into ascending or descending order, using the ORDER BY clause to specify the column to sort on.
* You can group query results by using the GROUP BY clause.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* Most databases come with built-in functions that you can use in SQL statements to perform operations on data within the database itself.
* When you work with large datasets, you may save time by using built-in functions rather than first retrieving the data into your 
  application and then executing functions on the retrieved data.
* You can use sub-queries to form more powerful queries than otherwise.
* You can use a sub-select expression to evaluate some built-in aggregate functions like the average function. 
* Derived tables or table expressions are sub-queries where the outer query uses the results of the sub-query as a data source.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* You can access a database from a language like Python by using the appropriate API. Examples include ibm_db API for IBM DB2, psycopg2 
  for ProstgreSQL, and dblib API for SQL Server.
* DB-API is Python's standard API for accessing relational databases. It allows you to write a single program that works with multiple 
  kinds of relational databases instead of writing a separate program for each one.
* The DB_API  connect constructor creates a connection to the database and returns a Connection Object, which is then used by the various 
  connection methods.
* The connection methods are:
  - The cursor() method, which returns a new cursor object using the connection.
  - The commit() method, which is used to commit any pending transaction to the database.
  - The rollback() method, which causes the database to roll-back to the start of any pending transaction.
  - The close() method, which is used to close a database connection. 
* You can use SQL Magic commands to execute queries more easily from Jupyter Notebooks. 
  - Magic commands have the general format %sql select * from tablename.
  - Cell magics start with a double %% (percent) sign and apply to the entire cell.
  - Line magics start with a single % (percent) sign and apply to a particular line in a cell.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* Views are a dynamic mechanism for presenting data from one or more tables.A transaction represents a complete unit of work, which can be 
  one or more SQL statements.
* An ACID transaction is one where all the SQL statements must complete successfully, or none at all.
* A stored procedure is a set of SQL statements that are stored and executed on the database server, allowing you to send one statement as 
  an alternative to sending multiple statements.
* You can write stored procedures in many different languages like SQL PL, PL/SQL, Java, and C.

## Summary & Highlights
Congratulations! You have completed this lesson. At this point in the course, you know:
* A join combines the rows from two or more tables based on a relationship between certain columns in these tables.
* To combine data from three or more different tables, you simply add new joins to the SQL statement. 
* There are two types of table joins: inner join and outer join; and three types of outer joins: left outer join, right outer join, and 
  full outer join. 
* The most common type of join is the inner join, which matches the results from two tables and returns only the rows that match.
* You can use an alias as shorthand for a table or column name.
* You can use a self-join to compare rows within the same table.

    ***

# Data Analysis using python <a id="3"></a>

## Lesson Summary
In this lesson, you have learned how to:
* **Define the Business Problem:** Look at the data and make some high-level decision on what kind of analysis should be done
* **Import and Export Data in Python:** How to import data from multiple data sources using the Pandas library and how to export files 
  into different formats.
* **Analyze Data in Python:** How to do some introductory analysis in Python using functions like dataframe.head() to view the first few 
  lines of the dataset, dataframe.info() to view the column names and data types.

## Lesson Summary
In this lesson, you have learned how to:
* **Identify and Handle Missing Values:** Drop rows with incomplete information and impute missing data using the mean values.
* **Understand Data Formatting:** Wrangle features in a dataset and make them meaningful for data analysis.
* **Apply normalization to a data set:** By understanding the relevance of using feature scaling on your data and how normalization and 
  standardization have varying effects on your data analysis.

## Lesson Summary
In this lesson, you have learned how to:
* **Describe Exploratory Data Analysis:** By summarizing the main characteristics of the data and extracting valuable insights.
* **Compute basic descriptive statistics:** Calculate the mean, median, and mode using python and use it as a basis in understanding the 
  distribution of the data.
* **Create data groups:** How and why you put continuous data in groups and how to visualize them.
* **Define correlation as the linear association between two numerical variables:** Use Pearson correlation as a measure of the 
  correlation between two continuous variables
* **Define the association between two categorical variables:** Understand how to find the association of two variables using the Chi- 
  square test for association and how to interpret them.

## Lesson Summary
In this lesson, you have learned how to:
* **Define the explanatory variable and the response variable:** Define the response variable (y) as the focus of the experiment and the 
  explanatory variable (x) as a variable used to explain the change of the response variable. Understand the differences between Simple 
  Linear Regression because it concerns the study of only one explanatory variable and Multiple Linear Regression because it concerns the 
  study of two or more explanatory variables.
* **Evaluate the model using Visualization:** By visually representing the errors of a variable using scatterplots and interpreting the 
  results of the model.
* **Identify alternative regression approaches:** Use a Polynomial Regression when the Linear regression does not capture the curvilinear 
  relationship between variables and how to pick the optimal order to use in a model.
* **Interpret the R-square and the Mean Square Error:** Interpret R-square (x 100) as the percentage of the variation in the response 
  variable y  that is explained by the variation in explanatory variable(s) x. The Mean Squared Error tells you how close a regression 
  line is to a set of points. It does this by taking the average distances from the actual points to the predicted points and squaring 
  them.


#### [Scroll To Top](#top)
